{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was code written while following along [notes from a Harvard lecture](https://docs.google.com/document/u/0/d/1VnNYGEmVgvl5p8w2xzypGySajaRv6qvzqw7E7LEwQKI/mobilebasic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "\n",
    "DATASET_PATH = os.environ.get(\"DATA_PATH\", \"data/\")\n",
    "\"\"\"\n",
    "pl.seed_everything seeds random, np.random, torch, and cuda\n",
    "\"\"\"\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "- After we import our libraries, we will use environment variables to get the path for our data. \n",
    "    - If the environment variable has not been defined, we will define it as \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop((32, 32), scale=(0.8, 1), ratio=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There's a competition strategy called **test time augmentations** where one \n",
    "can boost model performance by averaging the predictions over \n",
    "multiple augmented versions of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10(\n",
    "    root=DATASET_PATH, train=True, transform=train_transforms, download=True\n",
    ")\n",
    "val_dataset = CIFAR10(\n",
    "    root=DATASET_PATH, train=True, transform=test_transforms, download=True\n",
    ")\n",
    "test_dataset = CIFAR10(\n",
    "    root=DATASET_PATH, train=False, transform=test_transforms, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMGS = 4\n",
    "# Stack inserts a dimension\n",
    "# Select N images and stack them along their 0th dim\n",
    "# Results in (B, C, H, W)\n",
    "CIFAR_images = torch.stack(\n",
    "    [val_set[idx][0] for idx in range(NUM_IMGS), dim=0]\n",
    ")\n",
    "img_grid = torchvision.utils.make_grid(\n",
    "    CIFAR_images, nrow=NUM_IMGS, normalize=True, pad_value=0.9\n",
    ").permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaders\n",
    "\n",
    "- Combines a sampler and dataset. Iterates over the dataset and returns batches of a specified length.\n",
    "- `drop_last=True` removes the last incomplete batch if the number of samples is not divisible by the batch size.\n",
    "- `num_workers` determined by trial and error, stop when performance stops improving. Start with the number of cores available, may run out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.DataLoader(\n",
    "    train_set, batch_size=128, drop_last=True, pin_memory=True, num_workers=4\n",
    ")\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4\n",
    ")\n",
    "test_dataloader = data.DataLoader(\n",
    "    test_dataset, batch_size=128, shuffle=False, drop_last=False, workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "- ViT requires the image to be broken into a sequence of smaller patches.\n",
    "    - For preprocessing, we split the original 32 x 32 image into 4 x 4 patches, resulting in 8 x 8 patches. \n",
    "        - Batch and channel dimensions are untouched, we're only transforming the height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_patch(\n",
    "    x: torch.Tensor, patch_size: int, flatten_channels: bool = True\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"img_to_patch _summary_\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor[B, C, H, W]): Tensor representing image.\n",
    "        patch_size (int): Height and Width of the patches\n",
    "        flatten_channels (bool, optional): Whether to flatten the patches into a feature vector or return as image grid. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Patches as feature vector or image grid.\n",
    "    \"\"\"\n",
    "    B, C, W, H = x.shape\n",
    "    x = x.reshape(\n",
    "        B,\n",
    "        C,\n",
    "        torch.div(H, patch_size, rounding_mode=\"trunc\"),\n",
    "        patch_size,\n",
    "        torch.div(W, patch_size, rounding_mode=\"trunc\"),\n",
    "        patch_size,\n",
    "    )\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5)  # B, H', W', C, p_H, p_W\n",
    "    x = x.flatten(1, 2)  # B, H'*W', C, p_H, p_W\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2, 4)  # B, H'*W', C*p_H*p_W\n",
    "    return x\n",
    "\n",
    "\n",
    "img_patches = img_to_patch(CIFAR_images, patch_size=4, flatten_channels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(CIFAR_images.shape[0], 1, figsize=(14, 3))\n",
    "fig.suptitle(\"Images as sequences of patches\")\n",
    "for i in range(CIFAR_images.shape[0]):\n",
    "    img_grid = torchvision.utils.make_grid(\n",
    "        img_patches[i], nrow=64, normalize=True, pad_value=0.9\n",
    "    )\n",
    "    img_grid = img_grid.permute(1, 2, 0)\n",
    "    ax[i].imshow(img_grid)\n",
    "    ax[i].axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        hidden_dim,\n",
    "        num_channels,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        num_classes,\n",
    "        patch_size,\n",
    "        num_patches,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Layers\n",
    "        self.input_layer = nn.Linear(num_channels * (patch_size**2), embed_dim)\n",
    "        self.transformer = nn.Sequential(\n",
    "            *(\n",
    "                AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ),\n",
    "        )\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim), nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.pos_embedding[:, : T + 1]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "class ViT(pl.LightningModule):\n",
    "    def __init__(self, model_kwargs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = VisionTransformer(**model_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters, lr=self.hparams.lr)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, milestones=[100, 150], gamma=0.1\n",
    "        )\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        imgs, labels = batch\n",
    "        preds = self(imgs)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float.mean()\n",
    "        self.log(f\"{mode}_loss\", loss, prog_bar=True)\n",
    "        self.log(f\"{mode}_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/VisionTransformers/\")\n",
    "\n",
    "\n",
    "def train_model(**kwargs):\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"), fast_dev_run=5\n",
    "    )\n",
    "\n",
    "    pl.seed_everything(42)\n",
    "    model = ViT(**kwargs)\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    test_result = trainer.test(model, dataloaders=test_dataloader, verbose=False)\n",
    "    return model, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    \"embed_dim\": 256,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_layers\": 6,\n",
    "    \"patch_size\": 4,\n",
    "    \"num_channels\": 3,\n",
    "    \"num_patches\": 64,\n",
    "    \"num_classes\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "}\n",
    "\n",
    "model, results = train_model(model_kwargs=model_kwargs, lr=3e-4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cebf5405b501047cf35ae5370e8d990e8d72968d8545e34ed4b6e118365b7d0b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

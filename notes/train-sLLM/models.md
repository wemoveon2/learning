- Coding
	- ![CrystalCoder](https://huggingface.co/LLM360/CrystalCoder) - 7B, 1.4t tokens, outperforms llama2
	- ![BigCode](https://huggingface.co/bigcode) - Multiple LLMs of various sizes.
- Base
	- ![WizardLM](https://huggingface.co/WizardLM/WizardLM-70B-V1.0) - ![Coding variant](https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder), also variants for math.
	- ![Phi-2](https://huggingface.co/microsoft/phi-2) - 2.7b model outperforming bigger models.
	- ![Capybara](https://huggingface.co/NousResearch/Nous-Capybara-3B-V1.9) - 3B model using novel data synthesis technique. 
- Multimodal
	- ![Obsidian](https://huggingface.co/NousResearch/Obsidian-3B-V0.5) - Tuned variant of Capybara 3B 
 